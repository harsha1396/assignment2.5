1.CLUSTER:

A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment. 
Typically one machine in the cluster is designated as the NameNode and another machine the as JobTracker which are the masters. 
The rest of the machines in the cluster act as both DataNode and TaskTracker which are the slaves. 
Hadoop clusters are known for boosting the speed of data analysis applications. 
They also are highly scalable.
If a cluster's processing power is influenced by growing volumes of data, additional cluster nodes can be added to increase throughput. 
Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes, which ensures that the data is not lost if one node fails.

2.COMPONENTS OF HADOOP 1.X:

The core components of Hadoop are: 

Job Tracker:

Only one instance of this process runs on a master node same as Name Node.
Any MapReduce job is submitted to Job Tracker first.
Job Tracker checks for the location various file blocks used in MapReduce processing.
Than it initiates the separate tasks on various Data Nodes by communicating with Task Tracker Daemons.

Task Tracker:

This process has multiple instances running on the slave nodes.
It receives the job information from Job Tracker daemon and initiates a task on that slave node.
In most of the cases, Task Tracker initiates the task on the same node where there physical data block is present.
Same as Data Node daemon, this process also periodically sends heart bits to Job Tracker to make Job Tracker aware that slave process is running.

The other main components are Name Node, Secondary Name Node and Data Node.

Name Node:

There is only single instance of this process runs on a cluster and that is on a master node.
It is responsible for manage metadata about files distributed across the cluster.
It manages information like location of file blocks across cluster and it’s permission.
This process reads all the metadata from a file named fsimage and keeps it in memory.
After this process is started, it updates metadata for newly added or removed files in RAM.
It periodically writes the changes in one file called edits as edit logs.
This process is a heart of HDFS, if it is down HDFS is not accessible any more.

Secondary Name Node:

For this also, only single instance of this process runs on a cluster
This process can run on a master node or can run on a separate node depends on the size of the cluster.
It manages the metadata for the Name Node that is it reads the information written in edit logs and creates an updated file of current cluster metadata.
Than it transfers that file back to Name Node so that fsimage file can be updated.
So, whenever Name Node daemon is restarted it can always find updated information in fsimage file.

Data Node:

There are many instances of this process running on various slave nodes.
It is responsible for storing the individual file blocks on the slave nodes in Hadoop cluster.
Based on the replication factor, a single block is replicated in multiple slave nodes to prevent the data loss.
Whenever required, this process handles the access to a data block by communicating with Name Node.
This process periodically sends heart bits to Name Node to make Name Node aware that slave process is running.

 
